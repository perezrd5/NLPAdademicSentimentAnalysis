{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Part 2 - NLP Tasks\n",
    "## Doug Perez\n",
    "### 10/23/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Title:  Sentiment Analysis for Academic Reviews\n",
    "\n",
    "The goal of this project is to perform sentiment analysis of reviews of academic papers written by academic peers.  \n",
    "The dataset to be used is the Paper Reviews Data Set from UC-Irvine:\n",
    "https://archive.ics.uci.edu/ml/datasets/Paper+Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2:  Preprocessing\n",
    "\n",
    "In this section, we will normalize the data, remove stopwords, and split the data into training and test sets.\n",
    "Note:  the reviews contain both English and Spanish content.  As 382 of the 405 reviews are in Spanish, this analysis will focus exclusively on the Spanish reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "import nltk\n",
    "import textblob\n",
    "import re\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a couple of iterations of the analysis, I reached a couple of conclusions that helped boost the accuracy of the sentiment predictions.  The first is that I am using the Orientation column rather than the Evaluation column to predict the sentiment.  Orientation is a somewhat more subjective score based on a reading of the reviewer's comments.  The text is then evaluated from -2 to 2 based on how positive or negative the reviewers comments are.\n",
    "\n",
    "The second adjustment I made was to eliminate those reviews with an orientation score of 0.  These reviews are neither positive nor negative, and are thus not easy to include in a binary sentiment classification.  This adjustment lowered the number of reviews in the total sample to 278."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Spanish language reviews: 278\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>preliminary_decision</th>\n",
       "      <th>review</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>orientation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>accept</td>\n",
       "      <td>El artículo presenta recomendaciones prácticas...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>accept</td>\n",
       "      <td>- El tema es muy interesante y puede ser de mu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>accept</td>\n",
       "      <td>Se explica en forma ordenada y didáctica una e...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>accept</td>\n",
       "      <td>Este trabajo propone un nuevo enfoque basado e...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>accept</td>\n",
       "      <td>Se realiza un trabajo de modelamiento de encri...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id preliminary_decision                                             review  \\\n",
       "0   1               accept  El artículo presenta recomendaciones prácticas...   \n",
       "1   1               accept  - El tema es muy interesante y puede ser de mu...   \n",
       "2   2               accept  Se explica en forma ordenada y didáctica una e...   \n",
       "3   3               accept  Este trabajo propone un nuevo enfoque basado e...   \n",
       "4   4               accept  Se realiza un trabajo de modelamiento de encri...   \n",
       "\n",
       "  evaluation orientation  \n",
       "0          1           1  \n",
       "1          1           1  \n",
       "2          2           1  \n",
       "3          2           1  \n",
       "4          2           2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the json file and read the lines into a variable\n",
    "with open(\"./reviews.json\", \"r\", encoding='utf-8') as input_file:\n",
    "    data = json.load(input_file)\n",
    "\n",
    "# Create variable 'papers' for the top-level item of the json\n",
    "papers = data['paper']\n",
    "\n",
    "# Gather the id of the paper, its preliminary accept or reject decision, the review of the text, and the numeric evaluation\n",
    "paper_evaulations = [(paper['id'], paper['preliminary_decision'], review['text'], review['evaluation'], review['orientation'])\n",
    "                       for paper in papers for review in paper['review']\n",
    "                       if review['lan'] == 'es' and review['text'] and review['orientation'] != '0']\n",
    "\n",
    "# Print the count of reviews we will be working with\n",
    "print(f'Number of Spanish language reviews: {len(paper_evaulations)}')\n",
    "\n",
    "# Convert the list to a dataframe\n",
    "df = pd.DataFrame(paper_evaulations)\n",
    "df.columns = ['id','preliminary_decision', 'review','evaluation','orientation']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are going to tokenize the text and remove stopwords\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# Next, we define a function for normalizing the text:  making it lowercase, removing punctuation, removing spaces, etc.\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# Prepare reviews and evaluations (sentiments) for analysis\n",
    "reviews = np.array(df['review'])\n",
    "sentiments = np.array(df['orientation'])\n",
    "\n",
    "# Convert the sentiment from string to int\n",
    "int_sentiments = [int(sentiment) for sentiment in sentiments]\n",
    "# print(int_sentiments)\n",
    "\n",
    "# For ease of classification, convert the score from a range of -2 to 2 to a binary 'positive' or 'negative'.\n",
    "binary_sentiment = ['positive' if sentiment >= 0 else 'negative' for sentiment in int_sentiments]\n",
    "print(binary_sentiment[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194,) (84,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into test and training sets, then display the dimensions of the sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, binary_sentiment, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the reviews datasets\n",
    "X_train = normalize_corpus(X_train)\n",
    "X_test = normalize_corpus(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction\n",
    "\n",
    "In this section, we will use the Count Vectorizer and TF-IDF Vectorizer libraries to convert the text of the training and test sets into features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer and TfIdfVectorizer to build features from the reviews\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(X_train)\n",
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),\n",
    "                     sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(X_train)\n",
    "\n",
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(X_test)\n",
    "tv_test_features = tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (194, 19278)  Test features shape: (84, 19278)\n",
      "TFIDF model:> Train features shape: (194, 19278)  Test features shape: (84, 19278)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the features for CV and TV\n",
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Main Functionality\n",
    "\n",
    "In this section, we will train the models, use them to predict whether the review is positive or negative, then evaluate the performance of the different models.\n",
    "\n",
    "Specifically, we will use the Logistic Regression and SGD Classifier models to predict the sentiment of the review.\n",
    "\n",
    "We will apply the models to both the Bag of Words (BOW) and TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize the Logistic Regression and SGDClassifier models\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=500, C=1)\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100) # linear support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7381\n",
      "Precision: 0.7424\n",
      "Recall: 0.7381\n",
      "F1 Score: 0.7398\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.62      0.67      0.65        30\n",
      "    negative       0.81      0.78      0.79        54\n",
      "\n",
      "    accuracy                           0.74        84\n",
      "   macro avg       0.72      0.72      0.72        84\n",
      "weighted avg       0.74      0.74      0.74        84\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive         20       10\n",
      "        negative         12       42\n"
     ]
    }
   ],
   "source": [
    "# Run Logistic Regression model on BOW features and display performance\n",
    "lr_bow_predictions = meu.train_predict_model(classifier=lr, \n",
    "                                             train_features=cv_train_features, train_labels=y_train,\n",
    "                                             test_features=cv_test_features, test_labels=y_test)\n",
    "meu.display_model_performance_metrics(true_labels=y_test, predicted_labels=lr_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first model, linear regression, performed against the Bag of Words features accurately predicted the sentiment of the review about 74% of the time.  The model was somewhat more likely to issue a false positive prediction than a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.6429\n",
      "Precision: 0.4133\n",
      "Recall: 0.6429\n",
      "F1 Score: 0.5031\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.00      0.00      0.00        30\n",
      "    negative       0.64      1.00      0.78        54\n",
      "\n",
      "    accuracy                           0.64        84\n",
      "   macro avg       0.32      0.50      0.39        84\n",
      "weighted avg       0.41      0.64      0.50        84\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive          0       30\n",
      "        negative          0       54\n"
     ]
    }
   ],
   "source": [
    "# Run Logistic Regression model on TF-IDF features and display performance\n",
    "lr_tfidf_predictions = meu.train_predict_model(classifier=lr, \n",
    "                                               train_features=tv_train_features, train_labels=y_train,\n",
    "                                               test_features=tv_test_features, test_labels=y_test)\n",
    "meu.display_model_performance_metrics(true_labels=y_test, predicted_labels=lr_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the accuracy of this model, Linear Regression, run against the TF-IDF Vectorizer features performed only somewhat worse than the Linear Regression model run against Bag of Words, achieving an accuracy of almost 64%, it clearly has an issue with predicting positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7143\n",
      "Precision: 0.7143\n",
      "Recall: 0.7143\n",
      "F1 Score: 0.7143\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.60      0.60      0.60        30\n",
      "    negative       0.78      0.78      0.78        54\n",
      "\n",
      "    accuracy                           0.71        84\n",
      "   macro avg       0.69      0.69      0.69        84\n",
      "weighted avg       0.71      0.71      0.71        84\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive         18       12\n",
      "        negative         12       42\n"
     ]
    }
   ],
   "source": [
    "# Run SDGClassifier model on BOW features and display performance\n",
    "svm_bow_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                             train_features=cv_train_features, train_labels=y_train,\n",
    "                                             test_features=cv_test_features, test_labels=y_test)\n",
    "meu.display_model_performance_metrics(true_labels=y_test, predicted_labels=svm_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SDG Classifier run against the Bag of Words results in a modest drop in prediction accuracy.  Just over 71% of the predictions were accurate, with the same number of false negatives as false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7857\n",
      "Precision: 0.8041\n",
      "Recall: 0.7857\n",
      "F1 Score: 0.7654\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.88      0.47      0.61        30\n",
      "    negative       0.76      0.96      0.85        54\n",
      "\n",
      "    accuracy                           0.79        84\n",
      "   macro avg       0.82      0.71      0.73        84\n",
      "weighted avg       0.80      0.79      0.77        84\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive         14       16\n",
      "        negative          2       52\n"
     ]
    }
   ],
   "source": [
    "# Run SDGClassifier model on TF-IDF features and display performance\n",
    "svm_tfidf_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=tv_train_features, train_labels=y_train,\n",
    "                                                test_features=tv_test_features, test_labels=y_test)\n",
    "meu.display_model_performance_metrics(true_labels=y_test, predicted_labels=svm_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last model, the SDG Classifier run against the TF-IDF features the most accurate model with an accuracy of just under 79%.  This model featured the fewest false positives; however, it also featured the most false negatives (except for the clearly flawed SDG Classifier on TF-IDF), perhaps indicating that for sentiment analysis, this model is overly pessimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Personal Contribution Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is an individual project, the work is entirely my own.  I will therefore use this section to reflect on the work and discuss potential improvements.\n",
    "\n",
    "The accuracy of my predictions could be better.  I made several iterations of the analysis to achieve the accuracy displayed here.  First, I over simplified the sentiment score as a binary representation of positive or negative when the papers were actually evaluated from -2 to 2, with 0 clearly being a neutral score.  I chose to include 0 as a positive score.  I decided a better approach would be to eliminate the 0 evaluations and doing so would boost accuracy.  \n",
    "\n",
    "I then saw improved performance when I used the orientation column rather than the evaluation column.  Orientation was a subjective score of how positive or how negative a user's review actually seemed.  Using Orientation yielded more accurate predictions.\n",
    "\n",
    "The other factor that could affect the accuracy of the predictions is that this is my first attempt at sentiment analysis in a language other than English.  Although I used the Spanish language library in Spacy and Spanish stop words, I fear that I may need more time and experience working with NLP in another language before attempting sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
